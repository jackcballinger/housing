{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import json\n",
    "import numpy as np\n",
    "import time\n",
    "import datetime\n",
    "from datetime import timedelta\n",
    "import glob\n",
    "import os\n",
    "from sqlalchemy import create_engine\n",
    "from matplotlib import pyplot as plt\n",
    "from math import radians, cos, sin, asin, sqrt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "functions loaded\n"
     ]
    }
   ],
   "source": [
    "today = datetime.datetime.today().date()\n",
    "yesterday = datetime.datetime.today().date()-timedelta(1)\n",
    "\n",
    "def import_district_file(file_location):\n",
    "    district_df = pd.read_csv(file_location)\n",
    "    return district_df\n",
    "\n",
    "def get_district_dict():\n",
    "    district_df = import_district_file('C:/Users/ballinj/housing/london_district_codes.csv')\n",
    "    district_dict = dict(zip(list(district_df['district']), list(district_df['code'])))\n",
    "    return district_dict\n",
    "\n",
    "def get_borough_dict():\n",
    "    df = pd.read_csv('london_borough_list.csv')\n",
    "    borough_dict = dict(zip(list(df['borough']),list(df['code'])))\n",
    "    return borough_dict\n",
    "\n",
    "def get_no_results(soup):\n",
    "    no_results = soup.find('span', attrs={'class':'searchHeader-resultCount'}).text.strip()\n",
    "    return no_results\n",
    "\n",
    "def import_previous_file():\n",
    "    list_of_files = glob.glob('C:/Users/ballinj/housing/data/london/rightmove/*.csv')\n",
    "    latest_file = max(list_of_files, key=os.path.getctime)\n",
    "    combined_df_old = pd.read_csv(latest_file, index_col=False)\n",
    "    return combined_df_old\n",
    "\n",
    "def get_individual_soup(url):\n",
    "    cert = \"C:/Users/ballinj/housing/ca-certificates.crt\"\n",
    "    headers = {'User-Agent':'Mozilla/5.0 (Windows NT 6.2; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/32.0.1667.0 Safari/537.36'}\n",
    "    r = requests.get(url)\n",
    "    c = r.content    \n",
    "    soup = BeautifulSoup(c, 'html.parser')\n",
    "    return(soup)\n",
    "\n",
    "def get_soup(index, region):\n",
    "    district_id = district_dict[region]\n",
    "    cert = \"C:/Users/ballinj/housing/ca-certificates.crt\"\n",
    "    headers = {'User-Agent':'Mozilla/5.0 (Windows NT 6.2; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/32.0.1667.0 Safari/537.36'}\n",
    "    url = 'https://www.rightmove.co.uk/property-for-sale/find.html'\n",
    "    params = {'minBedrooms':1,\n",
    "              'propertyTypes':'detached%2Cflat%2Csemi-detached%2Cterraced',\n",
    "              'keywords':'',                  \n",
    "              'dontShow':'retirement%2CsharedOwnership',\n",
    "              'channel':'BUY',\n",
    "              'secondaryDisplayPropertyType':'housesandflats',\n",
    "              'index': str(index), \n",
    "              'retirement':'false',\n",
    "              'includeSSTC':'false',\n",
    "              'partBuyPartRent':'false',\n",
    "              'sortType':2,\n",
    "              'minPrice':200000,\n",
    "              'viewType':'list',\n",
    "              'maxPrice':450000,\n",
    "              'radius':0.0,\n",
    "              'locationIdentifier':'OUTCODE%' + str(district_id)[str(district_id).find('%')+1:]}\n",
    "    params_string = \"&\".join(\"%s=%s\" % (k,v) for k,v in params.items())\n",
    "    loaded = False\n",
    "    while not loaded:\n",
    "        r = requests.get(url, params=params_string)\n",
    "        c = r.content    \n",
    "        soup = BeautifulSoup(c, 'html.parser') \n",
    "        if soup.findAll('a', attrs={'class':'propertyCard-anchor'}) != None and soup.find('span', attrs={'class':'searchHeader-resultCount'}) != None:\n",
    "            loaded = True\n",
    "        else:\n",
    "            print('refreshing soup')\n",
    "    return soup\n",
    "\n",
    "def get_json(index, region):\n",
    "    district_id = district_dict[region]\n",
    "    cert = \"C:/Users/ballinj/housing/ca-certificates.crt\"\n",
    "    headers = {'User-Agent':'Mozilla/5.0 (Windows NT 6.2; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/32.0.1667.0 Safari/537.36'}\n",
    "    url = 'https://www.rightmove.co.uk/property-for-sale/map.html'\n",
    "    params = {'minBedrooms':1,\n",
    "              'propertyTypes':'detached%2Cflat%2Csemi-detached%2Cterraced',\n",
    "              'keywords':'',                  \n",
    "              'dontShow':'retirement%2CsharedOwnership',\n",
    "              'channel':'BUY',\n",
    "              'secondaryDisplayPropertyType':'housesandflats',\n",
    "              'index': str(index), \n",
    "              'retirement':'false',\n",
    "              'includeSSTC':'false',\n",
    "              'partBuyPartRent':'false',\n",
    "              'sortType':2,\n",
    "              'minPrice':200000,\n",
    "              'viewType':'map',\n",
    "              'maxPrice':450000,\n",
    "              'radius':0.0,\n",
    "              'locationIdentifier':'OUTCODE%' + str(district_id)[str(district_id).find('%')+1:]}\n",
    "    params_string = \"&\".join(\"%s=%s\" % (k,v) for k,v in params.items())\n",
    "    loaded = False\n",
    "    while not loaded:\n",
    "        r = requests.get(url, params=params_string)\n",
    "        c = r.content\n",
    "        soup = BeautifulSoup(c, 'html.parser')\n",
    "        scripts = soup.findAll('script')\n",
    "        script_list = [script if 'window.jsonModel' in str(script) else '' for script in scripts]\n",
    "        script_list = [script for script in script_list if script != '']\n",
    "        if len(script_list) != 0:\n",
    "            script = str(script_list[0])\n",
    "            script = script[script.find('{'):script.rfind('}')+1]\n",
    "            properties_json = json.loads(script)\n",
    "            properties_json = properties_json['properties']\n",
    "            loaded = True\n",
    "        else:\n",
    "            print('refreshing json')\n",
    "    print('json retrieved')\n",
    "    return properties_json\n",
    "\n",
    "def get_CR0_json(index):\n",
    "    district_id = district_dict['CR0']\n",
    "    cert = \"C:/Users/ballinj/housing/ca-certificates.crt\"\n",
    "    headers = {'User-Agent':'Mozilla/5.0 (Windows NT 6.2; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/32.0.1667.0 Safari/537.36'}\n",
    "    url = 'https://www.rightmove.co.uk/property-for-sale/map.html'\n",
    "    viewport_list = ['-0.24224%2C0.0873501%2C51.3716%2C51.4672','-0.240351%2C0.0892384%2C51.282%2C51.3777']\n",
    "    properties_json_list = []\n",
    "    for viewport in viewport_list:\n",
    "        params = {'minBedrooms':1,\n",
    "                  'propertyTypes':'detached%2Cflat%2Csemi-detached%2Cterraced',\n",
    "                  'keywords':'',                  \n",
    "                  'dontShow':'retirement%2CsharedOwnership',\n",
    "                  'channel':'BUY',\n",
    "                  'secondaryDisplayPropertyType':'housesandflats',\n",
    "                  'index': str(0), \n",
    "                  'retirement':'false',\n",
    "                  'includeSSTC':'false',\n",
    "                  'partBuyPartRent':'false',\n",
    "                  'sortType':2,\n",
    "                  'minPrice':200000,\n",
    "                  'viewType':'map',\n",
    "                  'maxPrice':450000,\n",
    "                  'radius':0.0,\n",
    "                  'locationIdentifier':'OUTCODE%' + str(district_id)[str(district_id).find('%')+1:],\n",
    "                  'viewport':viewport}\n",
    "        params_string = \"&\".join(\"%s=%s\" % (k,v) for k,v in params.items())\n",
    "        r = requests.get(url, params=params_string)\n",
    "        c = r.content\n",
    "        soup = BeautifulSoup(c, 'html.parser')\n",
    "        scripts = soup.findAll('script')\n",
    "        script_list = [script if 'window.jsonModel' in str(script) else '' for script in scripts]\n",
    "        script_list = [script for script in script_list if script != '']\n",
    "        script = str(script_list[0])\n",
    "        script = script[script.find('{'):script.rfind('}')+1]\n",
    "        properties_json = json.loads(script)\n",
    "        properties_json = properties_json['properties']\n",
    "        for p in properties_json:\n",
    "            if str(p['id']) not in [str(property['id']) for property in properties_json_list]:\n",
    "                properties_json_list.append(p)\n",
    "    return properties_json_list\n",
    "\n",
    "def format_data_soup(soup, region):\n",
    "    no_results = get_no_results(soup)\n",
    "    print(no_results + ' results found')\n",
    "    print('obtaining soup...')\n",
    "    index_array = np.arange(0,int(no_results)+24,24).tolist()\n",
    "    listing_ids, links, property_types, addresses, prices, featured_properties = [],[],[],[],[],[]\n",
    "    added_reduced_array, letting_agent_name, letting_agent_number, num_pictures = [],[],[],[]\n",
    "    for index in index_array:\n",
    "        soup = get_soup(index, region)\n",
    "        time.sleep(0.5)\n",
    "        main_data = soup.find('div', attrs={'class':'main'})\n",
    "        search_results = soup.find('div', attrs={'class':'l-searchResults'})\n",
    "        ids = soup.findAll('a', attrs={'class':'propertyCard-anchor'})#['id']\n",
    "        for id in ids:\n",
    "            listing_ids.append(id['id'][4:])\n",
    "        listing_data = search_results.findAll('div', attrs={'class':'propertyCard-wrapper'})\n",
    "        for listing in listing_data:\n",
    "            featured_properties.append(listing.find('div', attrs={'class':'propertyCard-moreInfoFeaturedTitle'}).text.strip())\n",
    "\n",
    "            details = listing.find('div', attrs={'class':'propertyCard-details'})\n",
    "            addresses.append(listing.find('address').text.strip())\n",
    "            property_types.append(listing.find('h2').text.strip())\n",
    "            links.append('https://www.rightmove.co.uk' + details.find('a')['href'])\n",
    "\n",
    "            pricing = listing.find('div', attrs={'class':'propertyCard-price'})\n",
    "            prices.append(pricing.find('div', attrs={'class':'propertyCard-priceValue'}).text.strip())\n",
    "            added_reduced_array.append(listing.find('div', attrs={'class':'propertyCard-branchSummary'}).find('span', attrs={'class':'propertyCard-branchSummary-addedOrReduced'}).text.strip())\n",
    "            estate_agent = listing.find('div', attrs={'class':'propertyCard-branchSummary'}).find('span', attrs={'class':'propertyCard-branchSummary-branchName'}).text.strip()\n",
    "            estate_agent = estate_agent[estate_agent.find('by')+3:].strip()\n",
    "            letting_agent_name.append(estate_agent)\n",
    "            letting_agent_number.append(listing.find('div', attrs={'class':'propertyCard-contacts'}).find('a', attrs={'class':'propertyCard-contactsPhoneNumber'}).text.strip())\n",
    "            meta_data = listing.find('div', attrs={'class':'propertyCard-moreInfoMeta'})\n",
    "            num_pictures.append(meta_data.find('span', attrs={'class':'propertyCard-moreInfoNumber'}).text.strip())\n",
    "    listing_df = pd.DataFrame(listing_ids, columns=['listing_id'])\n",
    "    listing_df['address'] = addresses\n",
    "    listing_df['property_type'] = property_types\n",
    "    listing_df['property_link'] = links\n",
    "    listing_df['price'] = prices\n",
    "    listing_df['added/reduced_date'] = added_reduced_array\n",
    "    listing_df['agent_name'] = letting_agent_name\n",
    "    listing_df['agent_number'] = letting_agent_number\n",
    "    listing_df['no_pictures'] = num_pictures\n",
    "    listing_df['featured_property'] = featured_properties\n",
    "    listing_df = listing_df[~listing_df['property_type'].str.contains('share')]\n",
    "    listing_df = listing_df[~listing_df['property_type'].str.contains('Parking')]\n",
    "    listing_df = listing_df[listing_df['address']!=\"\"]\n",
    "    listing_df = listing_df[listing_df['featured_property']==\"\"]\n",
    "    listing_df = listing_df.reset_index(drop=True)\n",
    "    return listing_df\n",
    "\n",
    "def format_missed_data(housing_soup_data, properties_json_list):\n",
    "    soup_list = housing_soup_data['listing_id'].tolist()\n",
    "    json_list = list(properties_json_list)\n",
    "    missed_ids, missed_links = [],[]\n",
    "    for item in json_list:\n",
    "        if item not in soup_list:\n",
    "            missed_ids.append(item)\n",
    "    missed_links = ['https://www.rightmove.co.uk/property-for-sale/property-' + str(item) + '.html' for item in missed_ids]\n",
    "    listing_ids, links, property_types, addresses, prices, featured_properties = [],[],[],[],[],[]\n",
    "    added_reduced_array, letting_agent_name, letting_agent_number, num_pictures = [],[],[],[]\n",
    "\n",
    "    for id,link in list(zip(missed_ids,missed_links))[:10]:\n",
    "        listing_ids.append(id)\n",
    "        links.append(link)\n",
    "        soup = get_individual_soup(link)\n",
    "        listing_details = soup.find('div', attrs={'id':'primaryContent'})\n",
    "        property_types.append(listing_details.find('h1', attrs={'class':'fs-22'}).text.strip())\n",
    "        addresses.append(listing_details.find('address', attrs={'itemprop':'address'}).text.strip())\n",
    "        prices.append(listing_details.find('p', attrs={'id':'propertyHeaderPrice'}).text.strip())\n",
    "        try:\n",
    "            added_reduced = soup.find('div', attrs={'id':'firstListedDate'}).text.strip().replace(' Rightmove:','') + ' '\n",
    "            added_reduced = added_reduced + datetime.datetime.strptime(soup.find('div', attrs={'id':'firstListedDateValue'}).text.strip(), '%d %B %Y').strftime('%d/%m/%Y')\n",
    "            added_reduced_array.append(added_reduced)\n",
    "        except AttributeError:\n",
    "            added_reduced_array.append(None)\n",
    "        letting_agent_name.append(soup.find('a', attrs={'id':'aboutBranchLink'}).text.strip())\n",
    "        letting_agent_number.append(soup.find('div', attrs={'id':'requestdetails'}).contents[4].contents[1].text.strip())\n",
    "        num_pictures.append(soup.find('span', attrs={'class':'gallery-main-status'}).text.strip()[-2:].strip())\n",
    "        time.sleep(1)\n",
    "    listing_df = pd.DataFrame(listing_ids, columns=['listing_id'])\n",
    "    listing_df['address'] = addresses\n",
    "    listing_df['property_type'] = property_types\n",
    "    listing_df['property_link'] = links\n",
    "    listing_df['price'] = prices\n",
    "    listing_df['added/reduced_date'] = added_reduced_array\n",
    "    listing_df['agent_name'] = letting_agent_name\n",
    "    listing_df['agent_number'] = letting_agent_number\n",
    "    listing_df['no_pictures'] = num_pictures\n",
    "    listing_df['featured_property'] = ''\n",
    "    listing_df = listing_df[~listing_df['property_type'].str.contains('share')]\n",
    "    listing_df = listing_df[~listing_df['property_type'].str.contains('Parking')]\n",
    "    listing_df = listing_df[listing_df['address']!=\"\"]\n",
    "    listing_df = listing_df.reset_index(drop=True)\n",
    "    housing_soup_data = pd.concat([housing_soup_data,listing_df], ignore_index=True)\n",
    "    return housing_soup_data\n",
    "    \n",
    "\n",
    "def format_data_json(properties_json):\n",
    "    property_id, coordinates = [],[]\n",
    "    for row in properties_json:\n",
    "        property_id.append(str(row['id']))\n",
    "        coordinates.append([row['location']['latitude'], row['location']['longitude']])\n",
    "    coordinates_dict = dict(zip(property_id, coordinates))\n",
    "    print('json formatted')\n",
    "    return coordinates_dict\n",
    "\n",
    "def housing_data_merge(housing_soup_data, housing_json_data):\n",
    "    latitudes, longitudes = [],[]\n",
    "    housing_soup_data = housing_soup_data[housing_soup_data['listing_id'].isin(list(housing_json_data.keys()))]\n",
    "    for id in housing_soup_data['listing_id'].tolist():\n",
    "        latitudes.append(housing_json_data[str(id)][0])\n",
    "        longitudes.append(housing_json_data[str(id)][1])\n",
    "    housing_soup_data['latitude'] = latitudes\n",
    "    housing_soup_data['longitude'] = longitudes\n",
    "    return housing_soup_data\n",
    "\n",
    "def format_housing_data_total(housing_data_total):\n",
    "    print(datetime.datetime.strftime(datetime.datetime.today(),'%H:%M:%S') + ' - formatting data')\n",
    "    price_list = [price.replace('Â£','').replace(',','') for price in housing_data_total['price'].tolist()]\n",
    "    housing_data_total.drop(columns=['price'])\n",
    "    housing_data_total['price'] = price_list\n",
    "    housing_data_total['most_recent_scrape_date'] = datetime.datetime.today().date()\n",
    "    room_df = pd.DataFrame(housing_data_total['property_type'].str.split(' bedroom ',1).tolist(),\n",
    "                                       columns = ['no_rooms','property_type'])\n",
    "    housing_data_total['added/reduced_date'] = housing_data_total['added/reduced_date'].fillna('unknown')\n",
    "    added_reduced_list = [ele.replace(' yesterday',' on ' + str(yesterday)).replace(' today',' on ' + str(today)) for ele in housing_data_total['added/reduced_date'].tolist()]\n",
    "    housing_data_total = housing_data_total.drop(columns=['added/reduced_date'])\n",
    "    housing_data_total['added/reduced_date'] = added_reduced_list\n",
    "    reduced_df = pd.DataFrame(housing_data_total['added/reduced_date'].str.split(' on ',1).tolist(),\n",
    "                                       columns = ['added/reduced','added/reduced_date'])\n",
    "    reduced_df['added/reduced_date'] = reduced_df['added/reduced_date'].fillna('unknown')\n",
    "    housing_data_total = housing_data_total.drop(columns=['property_type','added/reduced_date'])\n",
    "    housing_data_total = pd.concat([housing_data_total,room_df], axis=1)\n",
    "    housing_data_total = pd.concat([housing_data_total,reduced_df], axis=1)\n",
    "    housing_data_total['property_type'] = housing_data_total['property_type'].str.replace(' for sale','')\n",
    "    housing_data_total_old = import_previous_file()\n",
    "    initial_scrape_date = []\n",
    "    for index, row in housing_data_total.iterrows():\n",
    "        if int(row['listing_id']) not in housing_data_total_old['listing_id'].tolist():\n",
    "            initial_scrape_date.append(str(datetime.datetime.today().date()))\n",
    "        else:\n",
    "            required_date = housing_data_total_old[housing_data_total_old['listing_id']==int(row['listing_id'])]['initial_scrape_date'].tolist()[0]\n",
    "            initial_scrape_date.append(required_date)\n",
    "    housing_data_total['initial_scrape_date'] = initial_scrape_date\n",
    "    housing_data_total = housing_data_total[['listing_id',\n",
    "                                             'district',\n",
    "                                             'address',\n",
    "                                             'price',\n",
    "                                             'no_rooms',\n",
    "                                             'property_type',\n",
    "                                             'property_link',\n",
    "                                             'added/reduced',\n",
    "                                             'added/reduced_date',\n",
    "                                             'initial_scrape_date',\n",
    "                                             'most_recent_scrape_date',\n",
    "                                             'no_pictures',\n",
    "                                             'latitude',\n",
    "                                             'longitude',\n",
    "                                             'agent_name',\n",
    "                                             'agent_number']]\n",
    "    for index, row in housing_data_total_old.iterrows():\n",
    "        if int(row['listing_id']) not in [int(id) for id in housing_data_total['listing_id'].tolist()]:\n",
    "            housing_data_total = housing_data_total.append(row)\n",
    "    housing_data_total['added/reduced_date'] = housing_data_total['added/reduced_date'].apply(lambda x:datetime.datetime.strptime(str(x), '%d/%m/%Y') if '/' in str(x) else (datetime.datetime.strptime(str(x),'%Y-%m-%d') if '-' in str(x) else None))\n",
    "    housing_data_total['initial_scrape_date'] = housing_data_total['initial_scrape_date'].apply(lambda x:datetime.datetime.strptime(str(x), '%d/%m/%Y') if '/' in str(x) else (datetime.datetime.strptime(str(x),'%Y-%m-%d') if '-' in str(x) else None))\n",
    "    housing_data_total['most_recent_scrape_date'] = housing_data_total['most_recent_scrape_date'].apply(lambda x:datetime.datetime.strptime(str(x), '%d/%m/%Y') if '/' in str(x) else (datetime.datetime.strptime(str(x),'%Y-%m-%d') if '-' in str(x) else None))\n",
    "    housing_data_total = housing_data_total[~housing_data_total['property_link'].str.contains('commercial-property')]\n",
    "    housing_data_total = housing_data_total[housing_data_total['no_rooms'] != 'Hotel room for sale']\n",
    "    housing_data_total = housing_data_total.reset_index(drop=True)\n",
    "    print('data formatted')\n",
    "    return housing_data_total\n",
    "\n",
    "def haversine(lon1, lat1, lon2, lat2):\n",
    "    # convert decimal degrees to radians \n",
    "    lon1, lat1, lon2, lat2 = map(radians, [lon1, lat1, lon2, lat2])\n",
    "\n",
    "    # haversine formula \n",
    "    dlon = lon2 - lon1 \n",
    "    dlat = lat2 - lat1 \n",
    "    a = sin(dlat/2)**2 + cos(lat1) * cos(lat2) * sin(dlon/2)**2\n",
    "    c = 2 * asin(sqrt(a)) \n",
    "    r = 3956 # Radius of earth in kilometers = 6371. Use 3956 for miles\n",
    "    return c * r\n",
    "\n",
    "def add_stations(df):\n",
    "    print(datetime.datetime.strftime(datetime.datetime.today(),'%H:%M:%S') + ' - adding stations')\n",
    "    tube_stations = pd.read_csv('C:/Users/ballinj/housing/london_stations.csv', sep=',', index_col='FID')\n",
    "    distance_list,ids = [],[]\n",
    "    for id,lat, long in list(zip(df['listing_id'].tolist(),df['latitude'].tolist(),df['longitude'].tolist())):\n",
    "        distance_temp, distance_from_station_temp = [],[]\n",
    "        for index, row in tube_stations.iterrows():\n",
    "            distance_temp.append(haversine(long, lat, row['x'], row['y']))\n",
    "            distance_from_station_temp.append(row['NAME'])\n",
    "        zipped = zip(distance_temp,distance_from_station_temp)\n",
    "        distance_list.append(list(sorted(zipped, key=lambda x: x[0])[:3]))\n",
    "        ids.append(id)\n",
    "    distance_dict = dict(zip(ids,distance_list))\n",
    "    station_a_dist, station_b_dist, station_c_dist = [],[],[]\n",
    "    station_a, station_b, station_c = [],[],[]\n",
    "    for listing_id in df['listing_id'].tolist():\n",
    "        station_a_dist.append(distance_dict[listing_id][0][0])\n",
    "        station_b_dist.append(distance_dict[listing_id][1][0])\n",
    "        station_c_dist.append(distance_dict[listing_id][2][0])\n",
    "        station_a.append(distance_dict[listing_id][0][1])\n",
    "        station_b.append(distance_dict[listing_id][1][1])\n",
    "        station_c.append(distance_dict[listing_id][2][1])\n",
    "    df['closest_station'] = station_a\n",
    "    df['closest_station_dist (miles)'] = station_a_dist\n",
    "    df['second_closest_station'] = station_b\n",
    "    df['second_closest_station_dist (miles)'] = station_b_dist\n",
    "    df['third_closest_station'] = station_c\n",
    "    df['third_closest_station_dist (miles)'] = station_c_dist\n",
    "    print('stations added')\n",
    "    return df\n",
    "\n",
    "def add_pubs_score(housing_df):\n",
    "    \n",
    "    def format_total_normalised(row):\n",
    "        if row['quantity'] == 0:\n",
    "            return df['total_normalised'].min() - 0.01\n",
    "        else:\n",
    "            return row['total_normalised']\n",
    "    \n",
    "    print(datetime.datetime.strftime(datetime.datetime.today(),'%H:%M:%S') + ' - adding pubs')\n",
    "    relevant_pubs = pd.read_csv('C:/Users/ballinj/housing/pubs_data.csv', index_col=0)\n",
    "    relevant_pubs['rating'] = relevant_pubs['rating'].replace('unknown',0)\n",
    "    relevant_pubs['rating'] = pd.to_numeric(relevant_pubs['rating'])\n",
    "    relevant_pubs['rating'] = relevant_pubs['rating'].replace(0,relevant_pubs[relevant_pubs['rating']!=0]['rating'].mean())\n",
    "    property_pub_proximity_ratings, property_pub_average_ratings, property_pub_nos = [],[],[]\n",
    "    for lat, long in list(zip(housing_df['latitude'],housing_df['longitude'])):\n",
    "        pub_distances = []\n",
    "        for index, row in relevant_pubs.iterrows():\n",
    "            pub_distances.append(haversine(long, lat,row['long'],row['lat']))\n",
    "        pub_details = list(zip(pub_distances, relevant_pubs['rating'].tolist()))\n",
    "        near_pub_details = [row for row in pub_details if row[0] < 1]\n",
    "        if len(near_pub_details) > 0:\n",
    "            near_pub_average_rating = sum([row[1] for row in near_pub_details])/len(near_pub_details)\n",
    "            near_pub_proximity_rating = sum([row[0] for row in near_pub_details])/len(near_pub_details)\n",
    "        else:\n",
    "            near_pub_proximity_rating = 0\n",
    "            near_pub_average_rating = 0\n",
    "        property_pub_proximity_ratings.append(near_pub_proximity_rating)\n",
    "        property_pub_average_ratings.append(near_pub_average_rating)\n",
    "        property_pub_nos.append(len(near_pub_details))\n",
    "\n",
    "    list_df = list(zip(housing_df['listing_id'], property_pub_proximity_ratings, property_pub_nos, property_pub_average_ratings))\n",
    "    df = pd.DataFrame(list_df, columns=['listing_id','proximity','quantity','quality'])\n",
    "    df['proximity_normalised'] = df['proximity']-0.3 # here we presume that 300m is the optimal distance away from a pub\n",
    "    df['proximity_normalised'] = df['proximity_normalised'].abs() # here we penalised houses that are within 300m of a pub by taking the modulus\n",
    "    df['proximity_normalised'] = (df['proximity_normalised'] - df['proximity_normalised'].min())/(df['proximity_normalised'] - df['proximity_normalised'].min()).max()\n",
    "    df['quantity_normalised'] = df['quantity']/df['quantity'].max()\n",
    "    df['quality_normalised'] = (df['quality']-df[df['quality']!=0]['quality'].min())/(df['quality']-df[df['quality']!=0]['quality'].min()).max()\n",
    "    df['quality_normalised'][df['quality_normalised']<0] = 0\n",
    "    df['total_normalised'] = 0.5*df['quantity_normalised'] + 2*df['quality_normalised'] - df['proximity_normalised']\n",
    "    df['total_normalised'] = df.apply(lambda row: format_total_normalised(row), axis=1)\n",
    "    df['total_rank'] = df['total_normalised'].rank(ascending=False)\n",
    "    housing_df = housing_df.drop(columns=['listing_id.1', 'proximity',\n",
    "       'proximity_normalised', 'quality', 'quality_normalised', 'quantity',\n",
    "       'quantity_normalised', 'total_normalised', 'total_rank'])\n",
    "    housing_df = pd.concat([housing_df,df], axis=1)\n",
    "    return housing_df\n",
    "print('functions loaded')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "19:40:23.646927\n",
      "BR1\n",
      "195 results found\n",
      "obtaining soup...\n",
      "json retrieved\n",
      "json formatted\n",
      "BR1 complete\n",
      "1 out of 298\n",
      "\n",
      "\n",
      "BR2\n",
      "166 results found\n",
      "obtaining soup...\n",
      "json retrieved\n",
      "json formatted\n",
      "BR2 complete\n",
      "2 out of 298\n",
      "\n",
      "\n",
      "BR3\n",
      "110 results found\n",
      "obtaining soup...\n",
      "json retrieved\n",
      "json formatted\n",
      "BR3 complete\n",
      "3 out of 298\n",
      "\n",
      "\n",
      "BR4\n",
      "21 results found\n",
      "obtaining soup...\n",
      "json retrieved\n",
      "json formatted\n",
      "BR4 complete\n",
      "4 out of 298\n",
      "\n",
      "\n",
      "BR5\n",
      "148 results found\n",
      "obtaining soup...\n",
      "json retrieved\n",
      "json formatted\n",
      "BR5 complete\n",
      "5 out of 298\n",
      "\n",
      "\n",
      "BR6\n",
      "132 results found\n",
      "obtaining soup...\n",
      "json retrieved\n",
      "json formatted\n",
      "BR6 complete\n",
      "6 out of 298\n",
      "\n",
      "\n",
      "BR7\n",
      "58 results found\n",
      "obtaining soup...\n",
      "json retrieved\n",
      "json formatted\n",
      "BR7 complete\n",
      "7 out of 298\n",
      "\n",
      "\n",
      "BR8\n",
      "72 results found\n",
      "obtaining soup...\n",
      "json retrieved\n",
      "json formatted\n",
      "BR8 complete\n",
      "8 out of 298\n",
      "\n",
      "\n",
      "CM13\n",
      "60 results found\n",
      "obtaining soup...\n",
      "json retrieved\n",
      "json formatted\n",
      "CM13 complete\n",
      "9 out of 298\n",
      "\n",
      "\n",
      "CM14\n",
      "202 results found\n",
      "obtaining soup...\n",
      "json retrieved\n",
      "json formatted\n",
      "CM14 complete\n",
      "10 out of 298\n",
      "\n",
      "\n",
      "CR0\n",
      "938 results found\n",
      "obtaining soup...\n",
      "json formatted\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\ballinj\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\ipykernel_launcher.py:263: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "C:\\Users\\ballinj\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\ipykernel_launcher.py:264: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "C:\\Users\\ballinj\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\ipykernel_launcher.py:19: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CR0 complete\n",
      "11 out of 298\n",
      "\n",
      "\n",
      "CR2\n",
      "220 results found\n",
      "obtaining soup...\n",
      "json retrieved\n",
      "json formatted\n",
      "CR2 complete\n",
      "12 out of 298\n",
      "\n",
      "\n",
      "CR3\n",
      "160 results found\n",
      "obtaining soup...\n",
      "json retrieved\n",
      "json formatted\n",
      "CR3 complete\n",
      "13 out of 298\n",
      "\n",
      "\n",
      "CR4\n",
      "220 results found\n",
      "obtaining soup...\n",
      "json retrieved\n",
      "json formatted\n",
      "CR4 complete\n",
      "14 out of 298\n",
      "\n",
      "\n",
      "CR5\n",
      "57 results found\n",
      "obtaining soup...\n",
      "json retrieved\n",
      "json formatted\n",
      "CR5 complete\n",
      "15 out of 298\n",
      "\n",
      "\n",
      "CR6\n",
      "29 results found\n",
      "obtaining soup...\n",
      "json retrieved\n",
      "json formatted\n",
      "CR6 complete\n",
      "16 out of 298\n",
      "\n",
      "\n",
      "CR7\n",
      "222 results found\n",
      "obtaining soup...\n",
      "json retrieved\n",
      "json formatted\n",
      "CR7 complete\n",
      "17 out of 298\n",
      "\n",
      "\n",
      "CR8\n",
      "124 results found\n",
      "obtaining soup...\n",
      "json retrieved\n",
      "json formatted\n",
      "CR8 complete\n",
      "18 out of 298\n",
      "\n",
      "\n",
      "DA1\n",
      "205 results found\n",
      "obtaining soup...\n",
      "json retrieved\n",
      "json formatted\n",
      "DA1 complete\n",
      "19 out of 298\n",
      "\n",
      "\n",
      "DA14\n",
      "97 results found\n",
      "obtaining soup...\n",
      "json retrieved\n",
      "json formatted\n",
      "DA14 complete\n",
      "20 out of 298\n",
      "\n",
      "\n",
      "DA15\n",
      "103 results found\n",
      "obtaining soup...\n",
      "json retrieved\n",
      "json formatted\n",
      "DA15 complete\n",
      "21 out of 298\n",
      "\n",
      "\n",
      "DA16\n",
      "109 results found\n",
      "obtaining soup...\n",
      "json retrieved\n",
      "json formatted\n",
      "DA16 complete\n",
      "22 out of 298\n",
      "\n",
      "\n",
      "DA17\n",
      "79 results found\n",
      "obtaining soup...\n",
      "json retrieved\n",
      "json formatted\n",
      "DA17 complete\n",
      "23 out of 298\n",
      "\n",
      "\n",
      "DA18\n",
      "5 results found\n",
      "obtaining soup...\n",
      "json retrieved\n",
      "json formatted\n",
      "DA18 complete\n",
      "24 out of 298\n",
      "\n",
      "\n",
      "DA5\n",
      "48 results found\n",
      "obtaining soup...\n",
      "json retrieved\n",
      "json formatted\n",
      "DA5 complete\n",
      "25 out of 298\n",
      "\n",
      "\n",
      "DA6\n",
      "28 results found\n",
      "obtaining soup...\n",
      "json retrieved\n",
      "json formatted\n",
      "DA6 complete\n",
      "26 out of 298\n",
      "\n",
      "\n",
      "DA7\n",
      "84 results found\n",
      "obtaining soup...\n",
      "json retrieved\n",
      "json formatted\n",
      "DA7 complete\n",
      "27 out of 298\n",
      "\n",
      "\n",
      "DA8\n",
      "109 results found\n",
      "obtaining soup...\n",
      "json retrieved\n",
      "json formatted\n",
      "DA8 complete\n",
      "28 out of 298\n",
      "\n",
      "\n",
      "E1\n",
      "131 results found\n",
      "obtaining soup...\n",
      "json retrieved\n",
      "json formatted\n",
      "E1 complete\n",
      "29 out of 298\n",
      "\n",
      "\n",
      "E10\n",
      "102 results found\n",
      "obtaining soup...\n",
      "json retrieved\n",
      "json formatted\n",
      "E10 complete\n",
      "30 out of 298\n",
      "\n",
      "\n",
      "E11\n",
      "136 results found\n",
      "obtaining soup...\n",
      "json retrieved\n",
      "json formatted\n",
      "E11 complete\n",
      "31 out of 298\n",
      "\n",
      "\n",
      "E12\n",
      "69 results found\n",
      "obtaining soup...\n",
      "json retrieved\n",
      "json formatted\n",
      "E12 complete\n",
      "32 out of 298\n",
      "\n",
      "\n",
      "E13\n",
      "177 results found\n",
      "obtaining soup...\n",
      "json retrieved\n",
      "json formatted\n",
      "E13 complete\n",
      "33 out of 298\n",
      "\n",
      "\n",
      "E14\n",
      "385 results found\n",
      "obtaining soup...\n",
      "json retrieved\n",
      "json formatted\n",
      "E14 complete\n",
      "34 out of 298\n",
      "\n",
      "\n",
      "E15\n",
      "185 results found\n",
      "obtaining soup...\n",
      "json retrieved\n",
      "json formatted\n",
      "E15 complete\n",
      "35 out of 298\n",
      "\n",
      "\n",
      "E16\n",
      "275 results found\n",
      "obtaining soup...\n",
      "json retrieved\n",
      "json formatted\n",
      "E16 complete\n",
      "36 out of 298\n",
      "\n",
      "\n",
      "E17\n",
      "205 results found\n",
      "obtaining soup...\n",
      "json retrieved\n",
      "json formatted\n",
      "E17 complete\n",
      "37 out of 298\n",
      "\n",
      "\n",
      "E18\n",
      "61 results found\n",
      "obtaining soup...\n",
      "json retrieved\n",
      "json formatted\n",
      "E18 complete\n",
      "38 out of 298\n",
      "\n",
      "\n",
      "E1W\n",
      "26 results found\n",
      "obtaining soup...\n",
      "json retrieved\n",
      "json formatted\n",
      "E1W complete\n",
      "39 out of 298\n",
      "\n",
      "\n",
      "E2\n",
      "67 results found\n",
      "obtaining soup...\n",
      "json retrieved\n",
      "json formatted\n",
      "E2 complete\n",
      "40 out of 298\n",
      "\n",
      "\n",
      "E3\n",
      "206 results found\n",
      "obtaining soup...\n",
      "json retrieved\n",
      "json formatted\n",
      "E3 complete\n",
      "41 out of 298\n",
      "\n",
      "\n",
      "E4\n",
      "135 results found\n",
      "obtaining soup...\n",
      "json retrieved\n",
      "json formatted\n",
      "E4 complete\n",
      "42 out of 298\n",
      "\n",
      "\n",
      "E5\n",
      "68 results found\n",
      "obtaining soup...\n",
      "json retrieved\n",
      "json formatted\n",
      "E5 complete\n",
      "43 out of 298\n",
      "\n",
      "\n",
      "E6\n",
      "167 results found\n",
      "obtaining soup...\n",
      "json retrieved\n",
      "json formatted\n",
      "E6 complete\n",
      "44 out of 298\n",
      "\n",
      "\n",
      "E7\n",
      "69 results found\n",
      "obtaining soup...\n",
      "json retrieved\n",
      "json formatted\n",
      "E7 complete\n",
      "45 out of 298\n",
      "\n",
      "\n",
      "E8\n",
      "40 results found\n",
      "obtaining soup...\n",
      "json retrieved\n",
      "json formatted\n",
      "E8 complete\n",
      "46 out of 298\n",
      "\n",
      "\n",
      "E9\n",
      "64 results found\n",
      "obtaining soup...\n",
      "json retrieved\n",
      "json formatted\n",
      "E9 complete\n",
      "47 out of 298\n",
      "\n",
      "\n",
      "EC1A\n",
      "0 results found\n",
      "obtaining soup...\n",
      "json retrieved\n",
      "json formatted\n",
      "EC1A complete\n",
      "48 out of 298\n",
      "\n",
      "\n",
      "EC1M\n",
      "0 results found\n",
      "obtaining soup...\n",
      "json retrieved\n",
      "json formatted\n",
      "EC1M complete\n",
      "49 out of 298\n",
      "\n",
      "\n",
      "EC1N\n",
      "1 results found\n",
      "obtaining soup...\n",
      "json retrieved\n",
      "json formatted\n",
      "EC1N complete\n",
      "50 out of 298\n",
      "\n",
      "\n",
      "EC1R\n",
      "1 results found\n",
      "obtaining soup...\n",
      "json retrieved\n",
      "json formatted\n",
      "EC1R complete\n",
      "51 out of 298\n",
      "\n",
      "\n",
      "EC1V\n",
      "12 results found\n",
      "obtaining soup...\n",
      "json retrieved\n",
      "json formatted\n",
      "EC1V complete\n",
      "52 out of 298\n",
      "\n",
      "\n",
      "EC1Y\n",
      "1 results found\n",
      "obtaining soup...\n",
      "json retrieved\n",
      "json formatted\n",
      "EC1Y complete\n",
      "53 out of 298\n",
      "\n",
      "\n",
      "EC2A\n",
      "3 results found\n",
      "obtaining soup...\n",
      "json retrieved\n",
      "json formatted\n",
      "EC2A complete\n",
      "54 out of 298\n",
      "\n",
      "\n",
      "EC2M\n",
      "0 results found\n",
      "obtaining soup...\n",
      "json retrieved\n",
      "json formatted\n",
      "EC2M complete\n",
      "55 out of 298\n",
      "\n",
      "\n",
      "EC2N\n",
      "0 results found\n",
      "obtaining soup...\n",
      "json retrieved\n",
      "json formatted\n",
      "EC2N complete\n",
      "56 out of 298\n",
      "\n",
      "\n",
      "EC2R\n",
      "0 results found\n",
      "obtaining soup...\n",
      "json retrieved\n",
      "json formatted\n",
      "EC2R complete\n",
      "57 out of 298\n",
      "\n",
      "\n",
      "EC2V\n",
      "0 results found\n",
      "obtaining soup...\n",
      "json retrieved\n",
      "json formatted\n",
      "EC2V complete\n",
      "58 out of 298\n",
      "\n",
      "\n",
      "EC2Y\n",
      "0 results found\n",
      "obtaining soup...\n",
      "json retrieved\n",
      "json formatted\n",
      "EC2Y complete\n",
      "59 out of 298\n",
      "\n",
      "\n",
      "EC3A\n",
      "0 results found\n",
      "obtaining soup...\n",
      "json retrieved\n",
      "json formatted\n",
      "EC3A complete\n",
      "60 out of 298\n",
      "\n",
      "\n",
      "EC3M\n",
      "0 results found\n",
      "obtaining soup...\n",
      "json retrieved\n",
      "json formatted\n",
      "EC3M complete\n",
      "61 out of 298\n",
      "\n",
      "\n",
      "EC3N\n",
      "0 results found\n",
      "obtaining soup...\n",
      "json retrieved\n",
      "json formatted\n",
      "EC3N complete\n",
      "62 out of 298\n",
      "\n",
      "\n",
      "EC3R\n",
      "0 results found\n",
      "obtaining soup...\n",
      "json retrieved\n",
      "json formatted\n",
      "EC3R complete\n",
      "63 out of 298\n",
      "\n",
      "\n",
      "EC3V\n",
      "0 results found\n",
      "obtaining soup...\n",
      "json retrieved\n",
      "json formatted\n",
      "EC3V complete\n",
      "64 out of 298\n",
      "\n",
      "\n",
      "EC4A\n",
      "1 results found\n",
      "obtaining soup...\n",
      "json retrieved\n",
      "json formatted\n",
      "EC4A complete\n",
      "65 out of 298\n",
      "\n",
      "\n",
      "EC4M\n",
      "0 results found\n",
      "obtaining soup...\n",
      "json retrieved\n",
      "json formatted\n",
      "EC4M complete\n",
      "66 out of 298\n",
      "\n",
      "\n",
      "EC4N\n",
      "0 results found\n",
      "obtaining soup...\n",
      "json retrieved\n",
      "json formatted\n",
      "EC4N complete\n",
      "67 out of 298\n",
      "\n",
      "\n",
      "EC4R\n",
      "0 results found\n",
      "obtaining soup...\n",
      "json retrieved\n",
      "json formatted\n",
      "EC4R complete\n",
      "68 out of 298\n",
      "\n",
      "\n",
      "EC4V\n",
      "0 results found\n",
      "obtaining soup...\n",
      "json retrieved\n",
      "json formatted\n",
      "EC4V complete\n",
      "69 out of 298\n",
      "\n",
      "\n",
      "EC4Y\n",
      "0 results found\n",
      "obtaining soup...\n",
      "json retrieved\n",
      "json formatted\n",
      "EC4Y complete\n",
      "70 out of 298\n",
      "\n",
      "\n",
      "EN1\n",
      "176 results found\n",
      "obtaining soup...\n",
      "json retrieved\n",
      "json formatted\n",
      "EN1 complete\n",
      "71 out of 298\n",
      "\n",
      "\n",
      "EN2\n",
      "98 results found\n",
      "obtaining soup...\n",
      "json retrieved\n",
      "json formatted\n",
      "EN2 complete\n",
      "72 out of 298\n",
      "\n",
      "\n",
      "EN3\n",
      "206 results found\n",
      "obtaining soup...\n",
      "json retrieved\n",
      "json formatted\n",
      "EN3 complete\n",
      "73 out of 298\n",
      "\n",
      "\n",
      "EN4\n",
      "37 results found\n",
      "obtaining soup...\n",
      "json retrieved\n",
      "json formatted\n",
      "EN4 complete\n",
      "74 out of 298\n",
      "\n",
      "\n",
      "EN5\n",
      "105 results found\n",
      "obtaining soup...\n",
      "json retrieved\n",
      "json formatted\n",
      "EN5 complete\n",
      "75 out of 298\n",
      "\n",
      "\n",
      "EN6\n",
      "82 results found\n",
      "obtaining soup...\n",
      "json retrieved\n",
      "json formatted\n",
      "EN6 complete\n",
      "76 out of 298\n",
      "\n",
      "\n",
      "EN7\n",
      "68 results found\n",
      "obtaining soup...\n",
      "json retrieved\n",
      "json formatted\n",
      "EN7 complete\n",
      "77 out of 298\n",
      "\n",
      "\n",
      "EN8\n",
      "201 results found\n",
      "obtaining soup...\n",
      "json retrieved\n",
      "json formatted\n",
      "EN8 complete\n",
      "78 out of 298\n",
      "\n",
      "\n",
      "EN9\n",
      "99 results found\n",
      "obtaining soup...\n",
      "json retrieved\n",
      "json formatted\n",
      "EN9 complete\n",
      "79 out of 298\n",
      "\n",
      "\n",
      "HA0\n",
      "127 results found\n",
      "obtaining soup...\n",
      "json retrieved\n",
      "json formatted\n",
      "HA0 complete\n",
      "80 out of 298\n",
      "\n",
      "\n",
      "HA1\n",
      "197 results found\n",
      "obtaining soup...\n",
      "json retrieved\n",
      "json formatted\n",
      "HA1 complete\n",
      "81 out of 298\n",
      "\n",
      "\n",
      "HA2\n",
      "131 results found\n",
      "obtaining soup...\n",
      "json retrieved\n",
      "json formatted\n",
      "HA2 complete\n",
      "82 out of 298\n",
      "\n",
      "\n",
      "HA3\n",
      "149 results found\n",
      "obtaining soup...\n"
     ]
    }
   ],
   "source": [
    "start_time = str(datetime.datetime.now().time()) \n",
    "print(start_time)\n",
    "district_dict = get_district_dict()\n",
    "district_list = import_district_file('C:/Users/ballinj/housing/london_district_codes.csv')['district'].tolist()\n",
    "housing_data_total = pd.DataFrame()\n",
    "i = 1\n",
    "for district in district_list:\n",
    "    print(district)\n",
    "    housing_soup = get_soup(index=0, region=district)\n",
    "    housing_soup_data = format_data_soup(housing_soup, region=district)\n",
    "    if district != 'CR0':\n",
    "        housing_json = get_json(index=0, region=district)\n",
    "        housing_json_data = format_data_json(housing_json)\n",
    "    else:\n",
    "        housing_json = get_CR0_json(index=0)\n",
    "        housing_json_data = format_data_json(housing_json)\n",
    "        housing_soup_data = format_missed_data(housing_soup_data, housing_json_data)\n",
    "    housing_data_merge_df = housing_data_merge(housing_soup_data, housing_json_data)\n",
    "    housing_data_merge_df['district'] = district\n",
    "    housing_data_total = pd.concat([housing_data_total,housing_data_merge_df], ignore_index=True)\n",
    "    time.sleep(3)\n",
    "    print(district + ' complete')\n",
    "    print(str(i) + ' out of ' + str(len(district_list)))\n",
    "    print('\\n')\n",
    "    i += 1\n",
    "housing_data_total_formatted = format_housing_data_total(housing_data_total)\n",
    "housing_data_total_stations = add_stations(housing_data_total_formatted)\n",
    "housing_data_total_pubs = add_pubs_score(housing_data_total_stations)\n",
    "housing_data_total_pubs.to_csv('data/london/rightmove/properties_by_district_{}.csv'.format(str(today)), index=False)\n",
    "print('PROCESS COMPLETE')\n",
    "end_time = str(datetime.datetime.now().time()) \n",
    "print(end_time)\n",
    "time_taken = datetime.datetime.strptime(end_time, '%H:%M:%S.%f') - datetime.datetime.strptime(start_time, '%H:%M:%S.%f')\n",
    "time_taken = str(time_taken)\n",
    "print('time taken: ' + time_taken)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=(16,9))\n",
    "\n",
    "housing_data_total_pubs.groupby('district')['total_normalised'].mean().sort_values(ascending=False).plot.bar()\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'housing_data_total_pubs' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-1-9a101e1e27aa>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mhousing_data_total_pubs\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mto_csv\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'data/london/rightmove/properties_by_district_{}.csv'\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mstr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtoday\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mindex\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mFalse\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m: name 'housing_data_total_pubs' is not defined"
     ]
    }
   ],
   "source": [
    "housing_data_total_pubs.to_csv('data/london/rightmove/properties_by_district_{}.csv'.format(str(today)), index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "housing_data_"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
